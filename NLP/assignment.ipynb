{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d80add",
   "metadata": {},
   "source": [
    "# ğŸ§± ì„ë² ë”©ì„ ìœ„í•œ ì¤€ë¹„ì‚¬í•­\n",
    "\n",
    "ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€, ìˆ˜ì¹˜í™” ê³¼ì •ì—ì„œ ì–´íœ˜ë¥¼ ë§Œë“œëŠ” ì¼ì…ë‹ˆë‹¤.   \n",
    "ì¦‰, í–¥í›„ì— ì²˜ë¦¬í•  ëª¨ë“  ë‹¨ì–´ë“¤ì— ë²ˆí˜¸ë¥¼ ë¶™ì´ëŠ” ê²ƒì…ë‹ˆë‹¤.   \n",
    "ëª¨ë“  ë‹¨ì–´ë“¤ì— ë²ˆí˜¸ë¥¼ ë¶™ì´ê³  ë‚˜ë©´ ë‹¨ì–´ë“¤ì„ one-hot ë²¡í„°ë¡œ ë°”ê¿€ ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.   \n",
    "ì„ë² ë”© ë ˆì´ì–´ëŠ” ì…ë ¥ë°›ì€ ë²ˆí˜¸ë¥¼ one-hot ë²¡í„°ë¡œ ë°”ê¾¼ ë’¤ì— ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ê³±í•´ì„œ ì‹¤ìˆ˜ ë²¡í„°ë¡œ ë°”ê¾¸ì–´ ì¤ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c34fc0",
   "metadata": {},
   "source": [
    "## ğŸ”¥ **ë‹¤ì‹œ í•œ ë²ˆ, í•„ìˆ˜ ë£¨í‹´!**\n",
    "\n",
    "1. ë§ë­‰ì¹˜ ì¤€ë¹„\n",
    "2. í† í° ë¶„ì ˆ (tokenization)\n",
    "3. ìˆ˜ì¹˜í™” (numericalization)\n",
    "4. ë°°ì¹˜ (batch) êµ¬ì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a831e9",
   "metadata": {},
   "source": [
    "### 1. ë§ë­‰ì¹˜ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56d66dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora ëŠ” ë‹¤ë¥¸ ë¶„ë“¤ì´ ì—°êµ¬ ëª©ì ìœ¼ë¡œ ê³µìœ í•´ì£¼ì‹  ë§ë­‰ì¹˜ë“¤ì„\n",
      "    ì†ì‰½ê²Œ ë‹¤ìš´ë¡œë“œ, ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ë§Œì„ ì œê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "    ë§ë­‰ì¹˜ë“¤ì„ ê³µìœ í•´ ì£¼ì‹  ë¶„ë“¤ì—ê²Œ ê°ì‚¬ë“œë¦¬ë©°, ê° ë§ë­‰ì¹˜ ë³„ ì„¤ëª…ê³¼ ë¼ì´ì„¼ìŠ¤ë¥¼ ê³µìœ  ë“œë¦½ë‹ˆë‹¤.\n",
      "    í•´ë‹¹ ë§ë­‰ì¹˜ì— ëŒ€í•´ ìì„¸íˆ ì•Œê³  ì‹¶ìœ¼ì‹  ë¶„ì€ ì•„ë˜ì˜ description ì„ ì°¸ê³ ,\n",
      "    í•´ë‹¹ ë§ë­‰ì¹˜ë¥¼ ì—°êµ¬/ìƒìš©ì˜ ëª©ì ìœ¼ë¡œ ì´ìš©í•˜ì‹¤ ë•Œì—ëŠ” ì•„ë˜ì˜ ë¼ì´ì„¼ìŠ¤ë¥¼ ì°¸ê³ í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nsmc] download ratings_train.txt: 14.6MB [00:01, 11.3MB/s]                     \n",
      "[nsmc] download ratings_test.txt: 4.90MB [00:00, 10.4MB/s]                      \n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "from Korpora import Korpora\n",
    "\n",
    "this_dir = pathlib.Path().parent.resolve()\n",
    "corpus = Korpora.load(\"nsmc\", root_dir=f\"{this_dir}/data\").get_all_texts()\n",
    "train_corpus = corpus[:-1000]\n",
    "test_corpus = corpus[-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d941050b",
   "metadata": {},
   "source": [
    "### 2. í† í° ë¶„ì ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cde2c5-db4c-449d-9459-8cf11144cb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchtext==0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99461671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Dataset\n",
    "from torchtext.data import Example\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class TokenizedDataset(Dataset):\n",
    "\n",
    "    def __init__(self, corpus, processors):\n",
    "        examples = []\n",
    "        for example in tqdm(corpus, desc=\"ì •ë‹µ ìƒì„± ë° í† í° ë¶„ì ˆ ì¤‘ì…ë‹ˆë‹¤\"):\n",
    "            text_pair = (example, example)\n",
    "            examples.append(Example.fromlist(text_pair, processors))\n",
    "        super().__init__(examples, processors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70fdf790",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /home/piai/Using_Git_Directory/Hustar_Study_Document/NLP/data/nsmc/ratings_train.txt\n",
      "  input_format: \n",
      "  model_prefix: spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 10000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  â‡ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: /home/piai/Using_Git_Directory/Hustar_Study_Document/NLP/data/nsmc/ratings_train.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 150001 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=6937327\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=1627\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 150001 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 497474 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 150001\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 507496\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 507496 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=237723 obj=15.4449 num_tokens=1142098 num_tokens/piece=4.80432\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=214660 obj=14.3379 num_tokens=1149215 num_tokens/piece=5.35365\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=160869 obj=14.3835 num_tokens=1178549 num_tokens/piece=7.32614\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=160183 obj=14.3031 num_tokens=1179564 num_tokens/piece=7.36385\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=120130 obj=14.4494 num_tokens=1217180 num_tokens/piece=10.1322\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=119961 obj=14.3764 num_tokens=1218135 num_tokens/piece=10.1544\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=89970 obj=14.5237 num_tokens=1255092 num_tokens/piece=13.9501\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=89858 obj=14.4691 num_tokens=1255310 num_tokens/piece=13.9699\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=67393 obj=14.6746 num_tokens=1299399 num_tokens/piece=19.2809\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=67393 obj=14.6226 num_tokens=1299556 num_tokens/piece=19.2832\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=50544 obj=14.8886 num_tokens=1352670 num_tokens/piece=26.7622\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=50544 obj=14.8303 num_tokens=1353124 num_tokens/piece=26.7712\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=37908 obj=15.1359 num_tokens=1412275 num_tokens/piece=37.2553\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=37908 obj=15.0708 num_tokens=1413065 num_tokens/piece=37.2762\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=28431 obj=15.4091 num_tokens=1479282 num_tokens/piece=52.0306\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=28431 obj=15.3348 num_tokens=1479944 num_tokens/piece=52.0539\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=21323 obj=15.7032 num_tokens=1551196 num_tokens/piece=72.7475\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=21323 obj=15.6186 num_tokens=1551311 num_tokens/piece=72.7529\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=15992 obj=16.013 num_tokens=1626477 num_tokens/piece=101.706\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=15992 obj=15.9115 num_tokens=1626499 num_tokens/piece=101.707\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM"
     ]
    }
   ],
   "source": [
    "from sentencepiece import SentencePieceProcessor\n",
    "from sentencepiece import SentencePieceTrainer\n",
    "\n",
    "# Subword í† í° ë¶„ì ˆ ì•Œê³ ë¦¬ì¦˜(unigram language model)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "SentencePieceTrainer.train(input=f\"{this_dir}/data/nsmc/ratings_train.txt\", model_prefix=\"spm\",\n",
    "\t\t\t\t\t\t   vocab_size=10000)\n",
    "tokenizer = SentencePieceProcessor(model_file=\"./spm.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "820fc31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " sub_iter=0 size=11994 obj=16.336 num_tokens=1706845 num_tokens/piece=142.308\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=11994 obj=16.2339 num_tokens=1706899 num_tokens/piece=142.313\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=11000 obj=16.3661 num_tokens=1731061 num_tokens/piece=157.369\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=11000 obj=16.3367 num_tokens=1731079 num_tokens/piece=157.371\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: spm.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: spm.vocab\n",
      "ì •ë‹µ ìƒì„± ë° í† í° ë¶„ì ˆ ì¤‘ì…ë‹ˆë‹¤: 100%|â–ˆ| 199000/199000 [00:10<00:00, 19720.93it/\n",
      "ì •ë‹µ ìƒì„± ë° í† í° ë¶„ì ˆ ì¤‘ì…ë‹ˆë‹¤: 100%|â–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 20900.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Field\n",
    "\n",
    "processors = list()\n",
    "bpe_func = lambda raw_text: tokenizer.encode(' '.join(raw_text), out_type=str)\n",
    "processors.append((\"src\", Field(sequential=True, use_vocab=True,\n",
    "\t\t\t  \t                init_token=\"<bos>\", preprocessing=bpe_func,\n",
    "\t\t\t\t\t\t\t\tpad_token=\"<pad>\", unk_token=\"<unk>\",\n",
    "\t\t\t\t\t\t\t\tbatch_first=True)))\n",
    "processors.append((\"tgt\", Field(sequential=True, use_vocab=True,\n",
    "                                init_token=None,\n",
    "\t\t\t\t\t\t\t\teos_token=\"<eos>\", preprocessing=bpe_func,\n",
    "\t\t\t\t\t\t\t\tpad_token=\"<pad>\", unk_token=\"<unk>\",\n",
    "\t\t\t\t\t\t\t\tbatch_first=True)))\n",
    "\n",
    "train_dataset = TokenizedDataset(train_corpus, processors)\n",
    "test_dataset = TokenizedDataset(test_corpus, processors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d8be28",
   "metadata": {},
   "source": [
    "### 3. ìˆ˜ì¹˜í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da4381cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for processor in processors:\n",
    "    processor[1].build_vocab(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b245af",
   "metadata": {},
   "source": [
    "![embedding_dimension](https://www.tensorflow.org/text/guide/images/embedding2.png)\n",
    "\n",
    "ì´ì œ ì„ë² ë”© ë ˆì´ì–´ë¥¼ ë§Œë“¤ì–´ ë´…ì‹œë‹¤.   \n",
    "ìœ„ì˜ ê·¸ë¦¼ì„ êµ¬í˜„í•˜ë ¤ë©´ \"*embedding_dim*\" íŒŒë¼ë¯¸í„°ì— 4ë¥¼ ì£¼ë©´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9975e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "emb = nn.Embedding(len(processors[0][1].vocab), embedding_dim=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b266d3f9",
   "metadata": {},
   "source": [
    "ì„ë² ë”© ë ˆì´ì–´ì˜ í•µì‹¬ì€ **ê°€ì¤‘ì¹˜ í–‰ë ¬**ì…ë‹ˆë‹¤.   \n",
    "$ \\text{(one-hot ë²¡í„°)} \\times \\text{(ê°€ì¤‘ì¹˜ í–‰ë ¬)} = \\text{(ì„ë² ë””ë“œ ë²¡í„°)} $ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.   \n",
    "ê·¸ë ‡ë‹¤ë©´ ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ì¶œë ¥í•´ ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ca76422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.3231,  1.9964, -0.9644, -0.2543],\n",
      "        [ 1.7903,  0.6064,  0.4292,  0.4127],\n",
      "        [ 1.8166, -0.6263,  0.4808, -0.5154],\n",
      "        ...,\n",
      "        [ 0.4514, -0.0170, -1.8670,  0.0587],\n",
      "        [ 1.9650, -0.3014, -0.3282, -0.4154],\n",
      "        [-0.0903,  0.5793, -0.4691,  0.7717]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(emb.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b10cf0",
   "metadata": {},
   "source": [
    "### 4. ë°°ì¹˜ êµ¬ì„±\n",
    "\n",
    "ì´ì œ ë°°ì¹˜ë¥¼ êµ¬ì„±í•˜ê³  í•´ë‹¹ ë°°ì¹˜ ë‚´ ë‹¨ì–´ì˜ ì„ë² ë”© ë²¡í„°ë¥¼ ì¶œë ¥í•´ ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efada4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„ë² ë”©ì´ ì™„ë£Œëœ í…ì„œì˜ ì‚¬ì´ì¦ˆ: torch.Size([60, 63, 4])\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Iterator\n",
    "\n",
    "BATCH_SIZE = 60\n",
    "\n",
    "train_batches = Iterator(train_dataset, batch_size=BATCH_SIZE,\n",
    "\t\t\t\t\t\t repeat=False, shuffle=True, sort=False)\n",
    "test_batches = Iterator(test_dataset, batch_size=BATCH_SIZE,\n",
    "\t\t\t\t\t\trepeat=False, shuffle=False, sort=False)\n",
    "\n",
    "for i, batch in enumerate(test_batches):\n",
    "\tprint(f\"ì„ë² ë”©ì´ ì™„ë£Œëœ í…ì„œì˜ ì‚¬ì´ì¦ˆ: {emb(batch.src).shape}\")\n",
    "\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4628380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# CUDA APIë¥¼ í†µí•´ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  torch.device()ë¡œ ì¥ì¹˜ ì´ë¦„ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07b4b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class RNNModel(nn.Module): \n",
    "    \"\"\"\n",
    "    RNN ì–¸ì–´ ëª¨ë¸ í´ë˜ìŠ¤ (PyTorchì˜ nn.Moduleì„ ìƒì†ë°›ì•„ì„œ\n",
    "    ì‹ ê²½ë§ì„ ì •ì˜í•©ë‹ˆë‹¤.)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rep_dim, n_layers, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # ëª¨ë¸ì—ì„œ ì‚¬ìš©í•  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„ ì–¸í•©ë‹ˆë‹¤.\n",
    "        self.rep_dim = rep_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # ë ˆì´ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "        self.emb = nn.Embedding(vocab_size, embedding_dim=rep_dim)\n",
    "        # torch.size([ë°°ì¹˜ ë‚´ ì‹œí€€ìŠ¤ ê°œìˆ˜, ì‹œí€€ìŠ¤ (ìµœëŒ€) ê¸¸ì´, ì€ë‹‰ í‘œìƒì˜ ì°¨ì›])\n",
    "        self.rnn = nn.LSTM(rep_dim, rep_dim, n_layers, batch_first=True)\n",
    "        self.out = nn.ModuleList([nn.Linear(rep_dim, vocab_size),\n",
    "                                     nn.LogSoftmax(dim=-1)])\n",
    "        \n",
    "        # (í•™ìŠµí• ) íŒŒë¼ë¯¸í„°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\" ë ˆì´ì–´ ì•ˆì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. \"\"\"\n",
    "        init_range = 0.1\n",
    "        self.emb.weight.data.uniform_(-init_range, init_range)\n",
    "        self.out[0].weight.data.uniform_(-init_range, init_range)\n",
    "        self.out[0].bias.data.zero_()\n",
    "\n",
    "    def forward(self, inp, init_hid_rep): \n",
    "        \"\"\" ëª¨ë¸ í˜¸ì¶œì‹œ ì‹¤í–‰ë©ë‹ˆë‹¤. \n",
    "        ë°˜í™˜ê°’:\n",
    "            ê° íƒ€ì„ ìŠ¤í…ì˜ RNN ì¶œë ¥.\n",
    "        \"\"\"\n",
    "        # torch.size([ë°°ì¹˜ ë‚´ ì‹œí€€ìŠ¤ ê°œìˆ˜, ì‹œí€€ìŠ¤ (ìµœëŒ€) ê¸¸ì´, ì€ë‹‰ í‘œìƒì˜ ì°¨ì›])\n",
    "        embedded = self.emb(inp)\n",
    "        \n",
    "        # ``self.rnn``ì€ ê° íƒ€ì„ ìŠ¤í…ì˜ ìµœì¢… ë ˆì´ì–´ì˜ í‘œìƒê³¼\n",
    "        # (torch.size[ë°°ì¹˜ ë‚´ ì‹œí€€ìŠ¤ ê°œìˆ˜, ì‹œí€€ìŠ¤ (ìµœëŒ€) ê¸¸ì´, ì€ë‹‰ í‘œìƒì˜ ì°¨ì›]))\n",
    "        # ë ˆì´ì–´ë³„ ìµœì¢… íƒ€ì„ ìŠ¤í…ì˜ ì€ë‹‰ í‘œìƒì„\n",
    "        # (torch.size[ë ˆì´ì–´ ê°œìˆ˜, ë°°ì¹˜ ë‚´ ì‹œí€€ìŠ¤ ê°œìˆ˜, ì€ë‹‰ í‘œìƒì˜ ì°¨ì›]))\n",
    "        # ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "        last_layer_rep, last_hid_rep = self.rnn(embedded, init_hid_rep)\n",
    "        \n",
    "        # torch.size([(ë°°ì¹˜ ë‚´ ì‹œí€€ìŠ¤ ê°œìˆ˜ * ì‹œí€€ìŠ¤ (ìµœëŒ€) ê¸¸ì´), ì€ë‹‰ í‘œìƒì˜ ì°¨ì›])\n",
    "        prob = self.out[0](last_layer_rep).view(-1, self.vocab_size)\n",
    "        log_prob = self.out[1](prob)\n",
    "\n",
    "        return log_prob, last_hid_rep\n",
    "\n",
    "    def get_initial_hid_rep(self, batch_size):\n",
    "        # íŒŒë¼ë¯¸í„°ë¥¼ ì•„ë¬´ê±°ë‚˜ í•˜ë‚˜ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "        weight = next(self.parameters())\n",
    "        # ``new_zeros()`` ë©”ì†Œë“œëŠ” í•´ë‹¹ íŒŒë¼ë¯¸í„°ì™€ ë™ì¼í•œ\n",
    "        # `torch.dtype`ê³¼ `torch.device` ê°’ì„ ê°€ì§€ëŠ”,\n",
    "        # 0ìœ¼ë¡œ ì±„ì›Œì§„ torch.Tensorë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "        h_0 = weight.new_zeros(self.n_layers, batch_size, self.rep_dim)\n",
    "        c_0 = weight.new_zeros(self.n_layers, batch_size, self.rep_dim)\n",
    "        return h_0, c_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f01f6181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "REP_DIM = 256\n",
    "MAX_EPOCH = 1\n",
    "\n",
    "# ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "vocab_size = len(processors[0][1].vocab)\n",
    "model = RNNModel(REP_DIM, n_layers=2, vocab_size=vocab_size).to(device)\n",
    "\n",
    "# í•™ìŠµì— ì‚¬ìš©í•  ìµœì í™” ê¸°ë²•ê³¼ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "criterion = nn.NLLLoss(ignore_index=0, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fed92eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_acc(scores, target):\n",
    "    \"\"\" ëª¨ë¸ ì˜ˆì¸¡ì˜ ì •í™•ë„(%)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. \"\"\"\n",
    "    pred = scores.max(-1)[1]\n",
    "    non_pad = target.ne(0)\n",
    "    num_correct = pred.eq(target).masked_select(non_pad).sum().item() \n",
    "    num_non_pad = non_pad.sum().item()\n",
    "    return 100 * (num_correct / num_non_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fac76f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    mean_loss = []\n",
    "    mean_acc = []\n",
    "    start_time = time.time()\n",
    "    for batch in tqdm(train_batches, desc=\"í›ˆë ¨ ì¤‘ì…ë‹ˆë‹¤.\"):\n",
    "        src = batch.src.to(device)\n",
    "        tgt = batch.tgt.view(-1).to(device)\n",
    "        init_hid_rep = []\n",
    "        for hid_rep in model.get_initial_hid_rep(len(batch)):\n",
    "            init_hid_rep.append(hid_rep.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        log_prob, last_hid_rep = model(src, init_hid_rep)\n",
    "        # ì†ì‹¤ê°’ì„ ì—­ì „íŒŒí•©ë‹ˆë‹¤.\n",
    "        loss = criterion(log_prob, tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # í‰ê°€ë¥¼ ìœ„í•´ì„œ ê²°ê³¼ë¥¼ ê¸°ë¡í•©ë‹ˆë‹¤.\n",
    "        mean_loss.append(loss.item())\n",
    "        mean_acc.append(cal_acc(log_prob, tgt))\n",
    "    total_time = time.time() - start_time\n",
    "    mean_acc = statistics.mean(mean_acc)\n",
    "    mean_loss = statistics.mean(mean_loss)\n",
    "    return mean_loss, total_time, mean_acc\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    mean_loss = []\n",
    "    mean_acc = []\n",
    "    for batch in tqdm(test_batches, desc=\"í‰ê°€ ì¤‘ì…ë‹ˆë‹¤.\"):\n",
    "        src = batch.src.to(device)\n",
    "        tgt = batch.tgt.view(-1).to(device)\n",
    "        init_hid_rep = []\n",
    "        for hid_rep in model.get_initial_hid_rep(len(batch)):\n",
    "            init_hid_rep.append(hid_rep.to(device))\n",
    "        with torch.no_grad():\n",
    "            log_prob, last_hid_rep = model(src, init_hid_rep)\n",
    "            loss = criterion(log_prob, tgt)\n",
    "            mean_loss.append(loss.item())\n",
    "            mean_acc.append(cal_acc(log_prob, tgt))\n",
    "    mean_acc = statistics.mean(mean_acc)\n",
    "    mean_loss = statistics.mean(mean_loss)\n",
    "    return mean_loss, mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2c706c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "í›ˆë ¨ ì¤‘ì…ë‹ˆë‹¤.: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3317/3317 [01:41<00:00, 32.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    1 | times 101.962 |  loss: 1.598 | accuracy: 77.30\n",
      "save model at: ./model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, MAX_EPOCH + 1):\n",
    "    train_batches = Iterator(train_dataset, batch_size=BATCH_SIZE,\n",
    "                             repeat=False, shuffle=True, sort=False)\n",
    "    loss, elapsed_time, accuracy = train()\n",
    "    print('epoch {:4d} | times {:3.3f} |  loss: {:3.3f} | accuracy: {:3.2f}'.format(epoch, elapsed_time, loss, accuracy))\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        loss, accuracy = evaluate()\n",
    "        print('=' * 60)\n",
    "        print('Evaluation | loss: {:3.3f} | accuracy: {:3.2f}'.format(loss, accuracy))\n",
    "        print('=' * 60)\n",
    "\n",
    "with open('model.pt', 'wb') as f:\n",
    "    print('save model at: ./model.pt')\n",
    "    torch.save(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0c35b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model from: ./model.pt\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'tgt' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3658/2074292554.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load model from: ./model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'``ë‚˜ëŠ” ë¶ˆìŒí•œ ëŒ€í•™ì›ìƒì´ì—ìš”.``: {:3.3f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_seq_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ë‚˜ëŠ” ë¶ˆìŒí•œ ëŒ€í•™ì›ìƒì´ì—ìš”.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'``ë‚˜ëŠ” ë¶€ìœ í•œ ëŒ€í•™ì›ìƒì´ì—ìš”.``: {:3.3f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_seq_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ë‚˜ëŠ” ë¶€ìœ í•œ ëŒ€í•™ì›ìƒì´ì—ìš”.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3658/2074292554.py\u001b[0m in \u001b[0;36mpred_seq_prob\u001b[0;34m(seq)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# í† í°ë³„ ë¡œê·¸ í™•ë¥  ì‚°ì¶œ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mtok_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'tgt' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def pred_seq_prob(seq):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Question 1: ì •ë‹µ ë°ì´í„° í”„ë¡œì„¸ì‹±\n",
    "        src_seq = []\n",
    "        src_seq.append(processors[0][1].vocab.stoi[\"<bos>\"])\n",
    "        for src_tok in processors[0][1].preprocess(seq):\n",
    "            src_seq.append(processors[0][1].vocab.stoi[src_tok])\n",
    "        src = torch.tensor(src_seq, dtype=torch.long)\n",
    "        \n",
    "        tgt_seq = []\n",
    "        # ?\n",
    "        tgt_seq.append(processors[1][1].vocab.stoi[\"<eos>\"])\n",
    "\n",
    "        # ì€ë‹‰ í‘œìƒ ì´ˆê¸°í™”\n",
    "        init_hid_rep = []\n",
    "        for hid_rep in model.get_initial_hid_rep(1):\n",
    "            init_hid_rep.append(hid_rep.squeeze(1))\n",
    "\n",
    "        # Question 2: ë¡œê·¸ í™•ë¥  ì‚°ì¶œ\n",
    "        # ?\n",
    "\n",
    "        # í† í°ë³„ ë¡œê·¸ í™•ë¥  ì‚°ì¶œ\n",
    "        tgt = tgt.unsqueeze(-1)\n",
    "        tok_probabilities = torch.gather(log_prob, 1, tgt).tolist()\n",
    "\n",
    "        # Question 3. ë¡œê·¸ í™•ë¥ ì˜ í•©ì„ ê²°ê³¼ë¡œ ë°˜í™˜\n",
    "        # return ?\n",
    "\n",
    "# load saved model\n",
    "with open('./model.pt', 'rb') as f:\n",
    "    print('load model from: ./model.pt')\n",
    "    model = torch.load(f).to(\"cpu\")\n",
    "    print('``ë‚˜ëŠ” ë¶ˆìŒí•œ ëŒ€í•™ì›ìƒì´ì—ìš”.``: {:3.3f}'.format(pred_seq_prob(\"ë‚˜ëŠ” ë¶ˆìŒí•œ ëŒ€í•™ì›ìƒì´ì—ìš”.\")))\n",
    "    print('``ë‚˜ëŠ” ë¶€ìœ í•œ ëŒ€í•™ì›ìƒì´ì—ìš”.``: {:3.3f}'.format(pred_seq_prob(\"ë‚˜ëŠ” ë¶€ìœ í•œ ëŒ€í•™ì›ìƒì´ì—ìš”.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b202d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
