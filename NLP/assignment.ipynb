{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65d80add",
   "metadata": {},
   "source": [
    "# 🧱 임베딩을 위한 준비사항\n",
    "\n",
    "가장 중요한 것은, 수치화 과정에서 어휘를 만드는 일입니다.   \n",
    "즉, 향후에 처리할 모든 단어들에 번호를 붙이는 것입니다.   \n",
    "모든 단어들에 번호를 붙이고 나면 단어들을 one-hot 벡터로 바꿀 수 있게 됩니다.   \n",
    "임베딩 레이어는 입력받은 번호를 one-hot 벡터로 바꾼 뒤에 가중치 행렬을 곱해서 실수 벡터로 바꾸어 줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c34fc0",
   "metadata": {},
   "source": [
    "## 🔥 **다시 한 번, 필수 루틴!**\n",
    "\n",
    "1. 말뭉치 준비\n",
    "2. 토큰 분절 (tokenization)\n",
    "3. 수치화 (numericalization)\n",
    "4. 배치 (batch) 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a831e9",
   "metadata": {},
   "source": [
    "### 1. 말뭉치 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56d66dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nsmc] download ratings_train.txt: 14.6MB [00:01, 11.3MB/s]                     \n",
      "[nsmc] download ratings_test.txt: 4.90MB [00:00, 10.4MB/s]                      \n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "from Korpora import Korpora\n",
    "\n",
    "this_dir = pathlib.Path().parent.resolve()\n",
    "corpus = Korpora.load(\"nsmc\", root_dir=f\"{this_dir}/data\").get_all_texts()\n",
    "train_corpus = corpus[:-1000]\n",
    "test_corpus = corpus[-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d941050b",
   "metadata": {},
   "source": [
    "### 2. 토큰 분절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cde2c5-db4c-449d-9459-8cf11144cb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchtext==0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99461671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Dataset\n",
    "from torchtext.data import Example\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class TokenizedDataset(Dataset):\n",
    "\n",
    "    def __init__(self, corpus, processors):\n",
    "        examples = []\n",
    "        for example in tqdm(corpus, desc=\"정답 생성 및 토큰 분절 중입니다\"):\n",
    "            text_pair = (example, example)\n",
    "            examples.append(Example.fromlist(text_pair, processors))\n",
    "        super().__init__(examples, processors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70fdf790",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /home/piai/Using_Git_Directory/Hustar_Study_Document/NLP/data/nsmc/ratings_train.txt\n",
      "  input_format: \n",
      "  model_prefix: spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 10000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: /home/piai/Using_Git_Directory/Hustar_Study_Document/NLP/data/nsmc/ratings_train.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 150001 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=6937327\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=1627\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 150001 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 497474 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 150001\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 507496\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 507496 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=237723 obj=15.4449 num_tokens=1142098 num_tokens/piece=4.80432\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=214660 obj=14.3379 num_tokens=1149215 num_tokens/piece=5.35365\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=160869 obj=14.3835 num_tokens=1178549 num_tokens/piece=7.32614\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=160183 obj=14.3031 num_tokens=1179564 num_tokens/piece=7.36385\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=120130 obj=14.4494 num_tokens=1217180 num_tokens/piece=10.1322\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=119961 obj=14.3764 num_tokens=1218135 num_tokens/piece=10.1544\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=89970 obj=14.5237 num_tokens=1255092 num_tokens/piece=13.9501\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=89858 obj=14.4691 num_tokens=1255310 num_tokens/piece=13.9699\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=67393 obj=14.6746 num_tokens=1299399 num_tokens/piece=19.2809\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=67393 obj=14.6226 num_tokens=1299556 num_tokens/piece=19.2832\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=50544 obj=14.8886 num_tokens=1352670 num_tokens/piece=26.7622\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=50544 obj=14.8303 num_tokens=1353124 num_tokens/piece=26.7712\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=37908 obj=15.1359 num_tokens=1412275 num_tokens/piece=37.2553\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=37908 obj=15.0708 num_tokens=1413065 num_tokens/piece=37.2762\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=28431 obj=15.4091 num_tokens=1479282 num_tokens/piece=52.0306\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=28431 obj=15.3348 num_tokens=1479944 num_tokens/piece=52.0539\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=21323 obj=15.7032 num_tokens=1551196 num_tokens/piece=72.7475\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=21323 obj=15.6186 num_tokens=1551311 num_tokens/piece=72.7529\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=15992 obj=16.013 num_tokens=1626477 num_tokens/piece=101.706\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=15992 obj=15.9115 num_tokens=1626499 num_tokens/piece=101.707\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM"
     ]
    }
   ],
   "source": [
    "from sentencepiece import SentencePieceProcessor\n",
    "from sentencepiece import SentencePieceTrainer\n",
    "\n",
    "# Subword 토큰 분절 알고리즘(unigram language model)을 사용합니다.\n",
    "SentencePieceTrainer.train(input=f\"{this_dir}/data/nsmc/ratings_train.txt\", model_prefix=\"spm\",\n",
    "\t\t\t\t\t\t   vocab_size=10000)\n",
    "tokenizer = SentencePieceProcessor(model_file=\"./spm.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "820fc31f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " sub_iter=0 size=11994 obj=16.336 num_tokens=1706845 num_tokens/piece=142.308\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=11994 obj=16.2339 num_tokens=1706899 num_tokens/piece=142.313\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=11000 obj=16.3661 num_tokens=1731061 num_tokens/piece=157.369\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=11000 obj=16.3367 num_tokens=1731079 num_tokens/piece=157.371\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: spm.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: spm.vocab\n",
      "정답 생성 및 토큰 분절 중입니다: 100%|█| 199000/199000 [00:10<00:00, 19720.93it/\n",
      "정답 생성 및 토큰 분절 중입니다: 100%|███| 1000/1000 [00:00<00:00, 20900.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Field\n",
    "\n",
    "processors = list()\n",
    "bpe_func = lambda raw_text: tokenizer.encode(' '.join(raw_text), out_type=str)\n",
    "processors.append((\"src\", Field(sequential=True, use_vocab=True,\n",
    "\t\t\t  \t                init_token=\"<bos>\", preprocessing=bpe_func,\n",
    "\t\t\t\t\t\t\t\tpad_token=\"<pad>\", unk_token=\"<unk>\",\n",
    "\t\t\t\t\t\t\t\tbatch_first=True)))\n",
    "processors.append((\"tgt\", Field(sequential=True, use_vocab=True,\n",
    "                                init_token=None,\n",
    "\t\t\t\t\t\t\t\teos_token=\"<eos>\", preprocessing=bpe_func,\n",
    "\t\t\t\t\t\t\t\tpad_token=\"<pad>\", unk_token=\"<unk>\",\n",
    "\t\t\t\t\t\t\t\tbatch_first=True)))\n",
    "\n",
    "train_dataset = TokenizedDataset(train_corpus, processors)\n",
    "test_dataset = TokenizedDataset(test_corpus, processors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d8be28",
   "metadata": {},
   "source": [
    "### 3. 수치화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da4381cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for processor in processors:\n",
    "    processor[1].build_vocab(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b245af",
   "metadata": {},
   "source": [
    "![embedding_dimension](https://www.tensorflow.org/text/guide/images/embedding2.png)\n",
    "\n",
    "이제 임베딩 레이어를 만들어 봅시다.   \n",
    "위의 그림을 구현하려면 \"*embedding_dim*\" 파라미터에 4를 주면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9975e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "emb = nn.Embedding(len(processors[0][1].vocab), embedding_dim=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b266d3f9",
   "metadata": {},
   "source": [
    "임베딩 레이어의 핵심은 **가중치 행렬**입니다.   \n",
    "$ \\text{(one-hot 벡터)} \\times \\text{(가중치 행렬)} = \\text{(임베디드 벡터)} $이기 때문입니다.   \n",
    "그렇다면 가중치 행렬을 출력해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ca76422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.3231,  1.9964, -0.9644, -0.2543],\n",
      "        [ 1.7903,  0.6064,  0.4292,  0.4127],\n",
      "        [ 1.8166, -0.6263,  0.4808, -0.5154],\n",
      "        ...,\n",
      "        [ 0.4514, -0.0170, -1.8670,  0.0587],\n",
      "        [ 1.9650, -0.3014, -0.3282, -0.4154],\n",
      "        [-0.0903,  0.5793, -0.4691,  0.7717]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(emb.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b10cf0",
   "metadata": {},
   "source": [
    "### 4. 배치 구성\n",
    "\n",
    "이제 배치를 구성하고 해당 배치 내 단어의 임베딩 벡터를 출력해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efada4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩이 완료된 텐서의 사이즈: torch.Size([60, 63, 4])\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Iterator\n",
    "\n",
    "BATCH_SIZE = 60\n",
    "\n",
    "train_batches = Iterator(train_dataset, batch_size=BATCH_SIZE,\n",
    "\t\t\t\t\t\t repeat=False, shuffle=True, sort=False)\n",
    "test_batches = Iterator(test_dataset, batch_size=BATCH_SIZE,\n",
    "\t\t\t\t\t\trepeat=False, shuffle=False, sort=False)\n",
    "\n",
    "for i, batch in enumerate(test_batches):\n",
    "\tprint(f\"임베딩이 완료된 텐서의 사이즈: {emb(batch.src).shape}\")\n",
    "\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4628380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# CUDA API를 통해 GPU를 사용할 수 있는지 확인하고 torch.device()로 장치 이름을 가져옵니다.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07b4b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class RNNModel(nn.Module): \n",
    "    \"\"\"\n",
    "    RNN 언어 모델 클래스 (PyTorch의 nn.Module을 상속받아서\n",
    "    신경망을 정의합니다.)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rep_dim, n_layers, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # 모델에서 사용할 하이퍼파라미터를 선언합니다.\n",
    "        self.rep_dim = rep_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # 레이어를 생성합니다.\n",
    "        self.emb = nn.Embedding(vocab_size, embedding_dim=rep_dim)\n",
    "        # torch.size([배치 내 시퀀스 개수, 시퀀스 (최대) 길이, 은닉 표상의 차원])\n",
    "        self.rnn = nn.LSTM(rep_dim, rep_dim, n_layers, batch_first=True)\n",
    "        self.out = nn.ModuleList([nn.Linear(rep_dim, vocab_size),\n",
    "                                     nn.LogSoftmax(dim=-1)])\n",
    "        \n",
    "        # (학습할) 파라미터를 초기화합니다.\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\" 레이어 안의 파라미터를 초기화합니다. \"\"\"\n",
    "        init_range = 0.1\n",
    "        self.emb.weight.data.uniform_(-init_range, init_range)\n",
    "        self.out[0].weight.data.uniform_(-init_range, init_range)\n",
    "        self.out[0].bias.data.zero_()\n",
    "\n",
    "    def forward(self, inp, init_hid_rep): \n",
    "        \"\"\" 모델 호출시 실행됩니다. \n",
    "        반환값:\n",
    "            각 타임 스텝의 RNN 출력.\n",
    "        \"\"\"\n",
    "        # torch.size([배치 내 시퀀스 개수, 시퀀스 (최대) 길이, 은닉 표상의 차원])\n",
    "        embedded = self.emb(inp)\n",
    "        \n",
    "        # ``self.rnn``은 각 타임 스텝의 최종 레이어의 표상과\n",
    "        # (torch.size[배치 내 시퀀스 개수, 시퀀스 (최대) 길이, 은닉 표상의 차원]))\n",
    "        # 레이어별 최종 타임 스텝의 은닉 표상을\n",
    "        # (torch.size[레이어 개수, 배치 내 시퀀스 개수, 은닉 표상의 차원]))\n",
    "        # 반환합니다.\n",
    "        last_layer_rep, last_hid_rep = self.rnn(embedded, init_hid_rep)\n",
    "        \n",
    "        # torch.size([(배치 내 시퀀스 개수 * 시퀀스 (최대) 길이), 은닉 표상의 차원])\n",
    "        prob = self.out[0](last_layer_rep).view(-1, self.vocab_size)\n",
    "        log_prob = self.out[1](prob)\n",
    "\n",
    "        return log_prob, last_hid_rep\n",
    "\n",
    "    def get_initial_hid_rep(self, batch_size):\n",
    "        # 파라미터를 아무거나 하나 가져옵니다.\n",
    "        weight = next(self.parameters())\n",
    "        # ``new_zeros()`` 메소드는 해당 파라미터와 동일한\n",
    "        # `torch.dtype`과 `torch.device` 값을 가지는,\n",
    "        # 0으로 채워진 torch.Tensor를 생성합니다.\n",
    "        h_0 = weight.new_zeros(self.n_layers, batch_size, self.rep_dim)\n",
    "        c_0 = weight.new_zeros(self.n_layers, batch_size, self.rep_dim)\n",
    "        return h_0, c_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f01f6181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터를 정의합니다.\n",
    "REP_DIM = 256\n",
    "MAX_EPOCH = 1\n",
    "\n",
    "# 모델을 생성합니다.\n",
    "vocab_size = len(processors[0][1].vocab)\n",
    "model = RNNModel(REP_DIM, n_layers=2, vocab_size=vocab_size).to(device)\n",
    "\n",
    "# 학습에 사용할 최적화 기법과 손실 함수를 정의합니다.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "criterion = nn.NLLLoss(ignore_index=0, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fed92eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_acc(scores, target):\n",
    "    \"\"\" 모델 예측의 정확도(%)를 계산합니다. \"\"\"\n",
    "    pred = scores.max(-1)[1]\n",
    "    non_pad = target.ne(0)\n",
    "    num_correct = pred.eq(target).masked_select(non_pad).sum().item() \n",
    "    num_non_pad = non_pad.sum().item()\n",
    "    return 100 * (num_correct / num_non_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fac76f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    mean_loss = []\n",
    "    mean_acc = []\n",
    "    start_time = time.time()\n",
    "    for batch in tqdm(train_batches, desc=\"훈련 중입니다.\"):\n",
    "        src = batch.src.to(device)\n",
    "        tgt = batch.tgt.view(-1).to(device)\n",
    "        init_hid_rep = []\n",
    "        for hid_rep in model.get_initial_hid_rep(len(batch)):\n",
    "            init_hid_rep.append(hid_rep.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        log_prob, last_hid_rep = model(src, init_hid_rep)\n",
    "        # 손실값을 역전파합니다.\n",
    "        loss = criterion(log_prob, tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # 평가를 위해서 결과를 기록합니다.\n",
    "        mean_loss.append(loss.item())\n",
    "        mean_acc.append(cal_acc(log_prob, tgt))\n",
    "    total_time = time.time() - start_time\n",
    "    mean_acc = statistics.mean(mean_acc)\n",
    "    mean_loss = statistics.mean(mean_loss)\n",
    "    return mean_loss, total_time, mean_acc\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    mean_loss = []\n",
    "    mean_acc = []\n",
    "    for batch in tqdm(test_batches, desc=\"평가 중입니다.\"):\n",
    "        src = batch.src.to(device)\n",
    "        tgt = batch.tgt.view(-1).to(device)\n",
    "        init_hid_rep = []\n",
    "        for hid_rep in model.get_initial_hid_rep(len(batch)):\n",
    "            init_hid_rep.append(hid_rep.to(device))\n",
    "        with torch.no_grad():\n",
    "            log_prob, last_hid_rep = model(src, init_hid_rep)\n",
    "            loss = criterion(log_prob, tgt)\n",
    "            mean_loss.append(loss.item())\n",
    "            mean_acc.append(cal_acc(log_prob, tgt))\n",
    "    mean_acc = statistics.mean(mean_acc)\n",
    "    mean_loss = statistics.mean(mean_loss)\n",
    "    return mean_loss, mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2c706c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "훈련 중입니다.: 100%|███████████████████████| 3317/3317 [01:41<00:00, 32.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    1 | times 101.962 |  loss: 1.598 | accuracy: 77.30\n",
      "save model at: ./model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, MAX_EPOCH + 1):\n",
    "    train_batches = Iterator(train_dataset, batch_size=BATCH_SIZE,\n",
    "                             repeat=False, shuffle=True, sort=False)\n",
    "    loss, elapsed_time, accuracy = train()\n",
    "    print('epoch {:4d} | times {:3.3f} |  loss: {:3.3f} | accuracy: {:3.2f}'.format(epoch, elapsed_time, loss, accuracy))\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        loss, accuracy = evaluate()\n",
    "        print('=' * 60)\n",
    "        print('Evaluation | loss: {:3.3f} | accuracy: {:3.2f}'.format(loss, accuracy))\n",
    "        print('=' * 60)\n",
    "\n",
    "with open('model.pt', 'wb') as f:\n",
    "    print('save model at: ./model.pt')\n",
    "    torch.save(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0c35b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model from: ./model.pt\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'tgt' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3658/2074292554.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load model from: ./model.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'``나는 불쌍한 대학원생이에요.``: {:3.3f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_seq_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"나는 불쌍한 대학원생이에요.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'``나는 부유한 대학원생이에요.``: {:3.3f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_seq_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"나는 부유한 대학원생이에요.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3658/2074292554.py\u001b[0m in \u001b[0;36mpred_seq_prob\u001b[0;34m(seq)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# 토큰별 로그 확률 산출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mtok_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'tgt' referenced before assignment"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def pred_seq_prob(seq):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Question 1: 정답 데이터 프로세싱\n",
    "        src_seq = []\n",
    "        src_seq.append(processors[0][1].vocab.stoi[\"<bos>\"])\n",
    "        for src_tok in processors[0][1].preprocess(seq):\n",
    "            src_seq.append(processors[0][1].vocab.stoi[src_tok])\n",
    "        src = torch.tensor(src_seq, dtype=torch.long)\n",
    "        \n",
    "        tgt_seq = []\n",
    "        # ?\n",
    "        tgt_seq.append(processors[1][1].vocab.stoi[\"<eos>\"])\n",
    "\n",
    "        # 은닉 표상 초기화\n",
    "        init_hid_rep = []\n",
    "        for hid_rep in model.get_initial_hid_rep(1):\n",
    "            init_hid_rep.append(hid_rep.squeeze(1))\n",
    "\n",
    "        # Question 2: 로그 확률 산출\n",
    "        # ?\n",
    "\n",
    "        # 토큰별 로그 확률 산출\n",
    "        tgt = tgt.unsqueeze(-1)\n",
    "        tok_probabilities = torch.gather(log_prob, 1, tgt).tolist()\n",
    "\n",
    "        # Question 3. 로그 확률의 합을 결과로 반환\n",
    "        # return ?\n",
    "\n",
    "# load saved model\n",
    "with open('./model.pt', 'rb') as f:\n",
    "    print('load model from: ./model.pt')\n",
    "    model = torch.load(f).to(\"cpu\")\n",
    "    print('``나는 불쌍한 대학원생이에요.``: {:3.3f}'.format(pred_seq_prob(\"나는 불쌍한 대학원생이에요.\")))\n",
    "    print('``나는 부유한 대학원생이에요.``: {:3.3f}'.format(pred_seq_prob(\"나는 부유한 대학원생이에요.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b202d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
