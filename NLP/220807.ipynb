{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”¥ **ìì—°ì–´ì²˜ë¦¬ í•„ìˆ˜ ë£¨í‹´!**\n",
    "\n",
    "1. ë§ë­‰ì¹˜ ì¤€ë¹„\n",
    "2. í† í° ë¶„ì ˆ (tokenization)\n",
    "3. ìˆ˜ì¹˜í™” (numericalization)\n",
    "4. ë°°ì¹˜ (batch) êµ¬ì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ë§ë­‰ì¹˜ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from Korpora import Korpora\n",
    "\n",
    "this_dir = pathlib.Path().parent.resolve()\n",
    "corpus = Korpora.load(\"nsmc\", root_dir=f\"{this_dir}/data\").get_all_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì–´ë–»ê²Œ ìƒê²¼ëŠ”ì§€ 10ê°œë§Œ ë³¼ê¹Œìš”?\n",
    "print(corpus[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. í† í° ë¶„ì ˆ (Tokenization)\n",
    "\n",
    "í† í° ë¶„ì ˆ ë°©ì‹ì€ ì—¬ëŸ¬ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤.   \n",
    "ë‹¨ì–´ ë˜ëŠ” ê·¸ ì´ìƒì˜ í° ë‹¨ìœ„ë¶€í„°, ìëª¨ ìˆ˜ì¤€ì˜ ì‘ì€ ë‹¨ìœ„ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.   \n",
    "í˜„ì¬ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” í† í° ë¶„ì ˆ ë°©ë²•ì€ subword ë‹¨ìœ„ì˜ ì•Œê³ ë¦¬ì¦˜ë“¤ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¨¼ì €, SpaCy tokenizerë¶€í„° ì‚´í´ë´…ì‹œë‹¤.\n",
    "!python -m spacy download ko_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "\n",
    "processor = Field(\n",
    "\tsequential=True, use_vocab=True, init_token=\"<bos>\", eos_token=\"<eos>\",\n",
    "\ttokenize=\"spacy\", tokenizer_language=\"ko_core_news_sm\",\n",
    "\tpad_token=\"<pad>\", unk_token=\"<unk>\", batch_first=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Dataset\n",
    "from torchtext.data import Example\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class TokenizedDataset(Dataset):\n",
    "\n",
    "    def __init__(self, corpus, processor):\n",
    "        fields = [(\"text\", processor)]\n",
    "        examples = []\n",
    "        for movie_comment in tqdm(corpus, desc=\"í† í° ë¶„ì ˆ ì¤‘ì…ë‹ˆë‹¤\"):\n",
    "            each_example = [movie_comment]\n",
    "            examples.append(\n",
    "                # ``Example`` ê°ì²´ê°€ ìƒì„±ë˜ëŠ” ê³¼ì •ì—ì„œ í† í° ë¶„ì ˆì´ ì¼ì–´ë‚˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "                Example.fromlist(each_example, fields)\n",
    "            )\n",
    "        super().__init__(examples, fields)\n",
    "\n",
    "dataset = TokenizedDataset(corpus, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í† í° ë¶„ì ˆì´ ì˜ ë˜ì—ˆëŠ”ì§€ ë³¼ê¹Œìš”?\n",
    "print([dataset[i].text for i in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ‘¨â€ğŸ’» Byte-Pair Encoding ë°©ì‹ì˜ í† í° ë¶„ì ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sentencepiece import SentencePieceProcessor\n",
    "from sentencepiece import SentencePieceTrainer\n",
    "\n",
    "SentencePieceTrainer.train(input=f\"{this_dir}/data/nsmc/ratings_train.txt\", model_prefix=\"spm\",\n",
    "\t\t\t\t\t\t   vocab_size=10000)\n",
    "tokenizer = SentencePieceProcessor(model_file=\"./spm.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.encode(\"ì˜í™”ê°€ ì˜ ì•ˆë˜ë„ ì¢‹ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì—„ë³µë™ í•˜ë‚˜ë§Œ ê¸°ì–µí•´ì£¼ì„¸ìš”\", out_type=str)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_func = lambda raw_text: tokenizer.encode(' '.join(raw_text), out_type=str)\n",
    "processor = Field(\n",
    "\tsequential=True, use_vocab=True, init_token=\"<bos>\", eos_token=\"<eos>\",\n",
    "    preprocessing=bpe_func,\n",
    "\tpad_token=\"<pad>\", unk_token=\"<unk>\", batch_first=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TokenizedDataset(corpus, processor)\n",
    "print([dataset[i].text for i in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ìˆ˜ì¹˜í™”\n",
    "\n",
    "ì´í›„ ì–´ë–¤ ì²˜ë¦¬ë¥¼ í•˜ë“ , ë¬¸ìì—´ë³´ë‹¤ëŠ” ìˆ«ìë¥¼ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ë‚«ìŠµë‹ˆë‹¤.   \n",
    "ê° í† í°ì„ ì–´ë–»ê²Œ ìˆ«ìë¡œ ë°”ê¿€ê¹Œìš”?   \n",
    "ê°„ë‹¨íˆ, ëª¨ë“  ê³ ìœ í•œ í† í°ì˜ ê°œìˆ˜ê°€ $n$ì´ë¼ë©´, $0$ë¶€í„° $(n-1)$ê¹Œì§€ ë²ˆí˜¸ë¥¼ ë§¤ê¸°ë©´ ë  ê²ƒì…ë‹ˆë‹¤.   \n",
    "ì´ë ‡ê²Œ í† í°ê³¼ ë²ˆí˜¸ë¥¼ ì§ì§€ì–´ì£¼ëŠ” ê²ƒì´ ë°”ë¡œ 'ì–´íœ˜'ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.build_vocab(dataset)\n",
    "# ë§Œë“¤ì–´ì§„ ì–´íœ˜ëŠ” ì–¼ë§ˆë‚˜ í´ê¹Œìš”?\n",
    "print(f\"ì–´íœ˜ì— ë“±ì¬ëœ í† í° ê°œìˆ˜: {len(processor.vocab)}ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¸ë±ìŠ¤ ìˆœìœ¼ë¡œ ì •ë ¬ëœ í† í°ì„ 10ê°œë§Œ ë´…ì‹œë‹¤.\n",
    "print(processor.vocab.itos[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì´ë²ˆì—ëŠ” ë°˜ëŒ€ë¡œ, í† í°ì„ í•˜ë‚˜ ì •í•´ì„œ ê·¸ ì¸ë±ìŠ¤ë¥¼ í™•ì¸í•´ ë´…ì‹œë‹¤.\n",
    "token = \"â–ì˜í™”\"\n",
    "print(f\"``{token}``ì˜ ì¸ë±ìŠ¤: {processor.vocab.stoi[token]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ë°°ì¹˜ êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator\n",
    "\n",
    "batch_producer = Iterator(dataset, batch_size=10,\n",
    "\t\t\t\t\t\t  repeat=True, shuffle=False, sort=False)\n",
    "for i, batch in enumerate(batch_producer):\n",
    "\t# ``torchtext.data.Batch`` ê°ì²´ê°€ ìƒì„±ë˜ëŠ” ê³¼ì •ì—ì„œ ìˆ˜ì¹˜í™”ê°€ ì¼ì–´ë‚˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "\t# ê·¸ ê²°ê³¼, ``batch.text``ëŠ” ``torch.LongTensor``ê°€ ë©ë‹ˆë‹¤.\n",
    "\tprint(batch.text.tolist())\n",
    "\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4d39d7678f083d4112fab20b5fd04b42951bda7bf43fd2924e491550025a0a7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
