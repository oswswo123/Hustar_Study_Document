{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔥 **자연어처리 필수 루틴!**\n",
    "\n",
    "1. 말뭉치 준비\n",
    "2. 토큰 분절 (tokenization)\n",
    "3. 수치화 (numericalization)\n",
    "4. 배치 (batch) 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 말뭉치 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from Korpora import Korpora\n",
    "\n",
    "this_dir = pathlib.Path().parent.resolve()\n",
    "corpus = Korpora.load(\"nsmc\", root_dir=f\"{this_dir}/data\").get_all_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어떻게 생겼는지 10개만 볼까요?\n",
    "print(corpus[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 토큰 분절 (Tokenization)\n",
    "\n",
    "토큰 분절 방식은 여러 가지가 있습니다.   \n",
    "단어 또는 그 이상의 큰 단위부터, 자모 수준의 작은 단위도 가능합니다.   \n",
    "현재 가장 많이 사용되는 토큰 분절 방법은 subword 단위의 알고리즘들입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 먼저, SpaCy tokenizer부터 살펴봅시다.\n",
    "!python -m spacy download ko_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Field\n",
    "\n",
    "processor = Field(\n",
    "\tsequential=True, use_vocab=True, init_token=\"<bos>\", eos_token=\"<eos>\",\n",
    "\ttokenize=\"spacy\", tokenizer_language=\"ko_core_news_sm\",\n",
    "\tpad_token=\"<pad>\", unk_token=\"<unk>\", batch_first=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Dataset\n",
    "from torchtext.data import Example\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class TokenizedDataset(Dataset):\n",
    "\n",
    "    def __init__(self, corpus, processor):\n",
    "        fields = [(\"text\", processor)]\n",
    "        examples = []\n",
    "        for movie_comment in tqdm(corpus, desc=\"토큰 분절 중입니다\"):\n",
    "            each_example = [movie_comment]\n",
    "            examples.append(\n",
    "                # ``Example`` 객체가 생성되는 과정에서 토큰 분절이 일어나게 됩니다.\n",
    "                Example.fromlist(each_example, fields)\n",
    "            )\n",
    "        super().__init__(examples, fields)\n",
    "\n",
    "dataset = TokenizedDataset(corpus, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰 분절이 잘 되었는지 볼까요?\n",
    "print([dataset[i].text for i in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 👨‍💻 Byte-Pair Encoding 방식의 토큰 분절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sentencepiece import SentencePieceProcessor\n",
    "from sentencepiece import SentencePieceTrainer\n",
    "\n",
    "SentencePieceTrainer.train(input=f\"{this_dir}/data/nsmc/ratings_train.txt\", model_prefix=\"spm\",\n",
    "\t\t\t\t\t\t   vocab_size=10000)\n",
    "tokenizer = SentencePieceProcessor(model_file=\"./spm.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.encode(\"영화가 잘 안되도 좋습니다. 하지만 엄복동 하나만 기억해주세요\", out_type=str)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_func = lambda raw_text: tokenizer.encode(' '.join(raw_text), out_type=str)\n",
    "processor = Field(\n",
    "\tsequential=True, use_vocab=True, init_token=\"<bos>\", eos_token=\"<eos>\",\n",
    "    preprocessing=bpe_func,\n",
    "\tpad_token=\"<pad>\", unk_token=\"<unk>\", batch_first=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TokenizedDataset(corpus, processor)\n",
    "print([dataset[i].text for i in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 수치화\n",
    "\n",
    "이후 어떤 처리를 하든, 문자열보다는 숫자를 처리하는 것이 낫습니다.   \n",
    "각 토큰을 어떻게 숫자로 바꿀까요?   \n",
    "간단히, 모든 고유한 토큰의 개수가 $n$이라면, $0$부터 $(n-1)$까지 번호를 매기면 될 것입니다.   \n",
    "이렇게 토큰과 번호를 짝지어주는 것이 바로 '어휘'입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.build_vocab(dataset)\n",
    "# 만들어진 어휘는 얼마나 클까요?\n",
    "print(f\"어휘에 등재된 토큰 개수: {len(processor.vocab)}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스 순으로 정렬된 토큰을 10개만 봅시다.\n",
    "print(processor.vocab.itos[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이번에는 반대로, 토큰을 하나 정해서 그 인덱스를 확인해 봅시다.\n",
    "token = \"▁영화\"\n",
    "print(f\"``{token}``의 인덱스: {processor.vocab.stoi[token]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 배치 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import Iterator\n",
    "\n",
    "batch_producer = Iterator(dataset, batch_size=10,\n",
    "\t\t\t\t\t\t  repeat=True, shuffle=False, sort=False)\n",
    "for i, batch in enumerate(batch_producer):\n",
    "\t# ``torchtext.data.Batch`` 객체가 생성되는 과정에서 수치화가 일어나게 됩니다.\n",
    "\t# 그 결과, ``batch.text``는 ``torch.LongTensor``가 됩니다.\n",
    "\tprint(batch.text.tolist())\n",
    "\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4d39d7678f083d4112fab20b5fd04b42951bda7bf43fd2924e491550025a0a7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
